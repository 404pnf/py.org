<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>wget下载页面链接到其它站点的图片 文件</title>
    <link href="/css/local.css" rel="stylesheet">
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-3301463-1']);
    _gaq.push(['_trackPageview']);
   (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
    </script>
    </head>
    <body>
      <h1>wget下载页面链接到其它站点的图片 文件</h1>

<p>wget下载页面链接到其它站点的图片/文件</p>

<h2>wget的-p选项</h2>
<blockquote>
  <p>–page-requisites: get all the elements that compose the page (images, CSS and so on).
http://www.linuxjournal.com/content/downloading-entire-web-site-wget</p>
</blockquote>

<p>就是我想要的。因为经常一个页面的图片是链接其它站点的。我以前读man怎么没看到这个选项。需要重读。</p>

<p>经过实验，这个参数也是只下载本域名下的链接文件，比如css，图像。如果你下载domain1.org的一个页面，上面链接了domain2.org的图片，用这个参数也不会抓下来domain2.org图片。</p>

<p>仔细读man相关部分发现：</p>

<pre><code>       Note that Wget will behave as if -r had been specified, but only that single page and its requisites will be
       downloaded.  Links from that page to external documents will not be followed.  Actually, to download a single
       page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly
       locally, this author likes to use a few options in addition to -p:

               wget -E -H -k -K -p http:// / To finish off this topic, it's worth knowing that Wget's idea of an external document link is any URL speci-
       fied in an " [" tag, an " " tag, or a " " tag other than " ".
</code></pre>

<h2>原来重点在 -h 就是 –span-hosts这个开关</h2>
<pre><code>   -H
   --span-hosts
       Enable spanning across hosts when doing recursive retrieving.
</code></pre>

<p>加上-H，就会去下载链接到domian2.org的图像了。哈哈</p>

<p>全明白了。</p>

<h2>最后答案</h2>

<p>wget -H -nc -nd -p 
基本就可以了。我还没有做更多试验。我抓一个页面和它页面上所有文件（这些文件和图片是存在其他站点上的）已经成功了。</p>

<p>如果你需要在本地浏览这些页面，别忘了加上 -EekK 之类的。</p>

<p>自己看一下man这些参数什么意思。很好记的。
## 看来man page要有机会就读
每次都有收获。
～]()</p>

    </body>
    <footer>
    <a href="../">Up    </a>
    <a href="/">Home</a>
    <!-- generated on 2013-01-23 09:55:45 +0800 -->
    </footer>
</html>

