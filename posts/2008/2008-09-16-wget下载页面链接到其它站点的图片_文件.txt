# wget下载页面链接到其它站点的图片 文件

wget下载页面链接到其它站点的图片/文件

## wget的-p选项
> --page-requisites: get all the elements that compose the page (images, CSS and so on).
> http://www.linuxjournal.com/content/downloading-entire-web-site-wget

就是我想要的。因为经常一个页面的图片是链接其它站点的。我以前读man怎么没看到这个选项。需要重读。

经过实验，这个参数也是只下载本域名下的链接文件，比如css，图像。如果你下载domain1.org的一个页面，上面链接了domain2.org的图片，用这个参数也不会抓下来domain2.org图片。

仔细读man相关部分发现：

           Note that Wget will behave as if -r had been specified, but only that single page and its requisites will be
           downloaded.  Links from that page to external documents will not be followed.  Actually, to download a single
           page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly
           locally, this author likes to use a few options in addition to -p:

                   wget -E -H -k -K -p http://
/
To finish off this topic, it's worth knowing that Wget's idea of an external document link is any URL speci-
           fied in an "
[" tag, an "
" tag, or a "
" tag other than "
".


## 原来重点在 -h 就是 --span-hosts这个开关
       -H
       --span-hosts
           Enable spanning across hosts when doing recursive retrieving.

加上-H，就会去下载链接到domian2.org的图像了。哈哈

全明白了。


## 最后答案

wget -H -nc -nd -p 
基本就可以了。我还没有做更多试验。我抓一个页面和它页面上所有文件（这些文件和图片是存在其他站点上的）已经成功了。

如果你需要在本地浏览这些页面，别忘了加上 -EekK 之类的。

自己看一下man这些参数什么意思。很好记的。
## 看来man page要有机会就读
每次都有收获。
～]()
